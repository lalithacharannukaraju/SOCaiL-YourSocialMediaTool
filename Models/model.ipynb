{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.115.6)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.34.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyngrokQ (from versions: none)\n",
      "ERROR: No matching distribution found for pyngrokQ\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install fastapi uvicorn transformers sentence-transformers scikit-learn pydantic huggingface_hub pyngrokQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from pyngrok import ngrok\n",
    "login(token=\"YOUR_HF_TOKEN\")\n",
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(\"YOUR_NGROK_TOKEN\")\n",
    "\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    query: str\n",
    "\n",
    "class SocialMediaRAG:\n",
    "    def __init__(self, trends_df: pd.DataFrame, songs_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with DataFrame inputs instead of CSV paths.\n",
    "\n",
    "        Args:\n",
    "            trends_df: DataFrame containing trends data\n",
    "            songs_df: DataFrame containing songs data\n",
    "        \"\"\"\n",
    "        print(\"Initializing models...\")\n",
    "        # Initialize FLAN-T5 model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"google/gemma-2-2b\",\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        # Initialize sentence transformer\n",
    "        print(\"Loading sentence transformer...\")\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        # Store DataFrames\n",
    "        self.trends_df = trends_df\n",
    "        self.songs_df = songs_df\n",
    "\n",
    "        # Create embeddings\n",
    "        print(\"Creating embeddings...\")\n",
    "        self._create_embeddings()\n",
    "        print(\"Initialization complete!\")\n",
    "\n",
    "    def _create_embeddings(self):\n",
    "        \"\"\"Create and store embeddings for all content\"\"\"\n",
    "        # Process trends\n",
    "        self.trends_texts = []\n",
    "        for _, row in self.trends_df.iterrows():\n",
    "            trend_name = row['Trend Name'].split('|')[0].strip() if pd.notna(row['Trend Name']) else \"\"\n",
    "            explanation = row['Explanation'] if pd.notna(row['Explanation']) else \"\"\n",
    "            trend_text = f\"Trend: {trend_name} | {explanation}\"\n",
    "            self.trends_texts.append(trend_text)\n",
    "\n",
    "        # Process songs\n",
    "        self.songs_texts = []\n",
    "        for _, row in self.songs_df.iterrows():\n",
    "            song_name = row['song_name'] if pd.notna(row['song_name']) else \"\"\n",
    "            artist = row['artist_name'] if pd.notna(row['artist_name']) else \"\"\n",
    "            reels_count = row['reels_count'] if pd.notna(row['reels_count']) else \"0\"\n",
    "            description = row['description'] if pd.notna(row['description']) else \"\"\n",
    "            \n",
    "            song_text = f\"Song: {song_name} by {artist} | Reels: {reels_count} | {description}\"\n",
    "            self.songs_texts.append(song_text)\n",
    "\n",
    "        # Combine all texts\n",
    "        self.all_texts = self.trends_texts + self.songs_texts\n",
    "\n",
    "        # Create embeddings\n",
    "        self.content_embeddings = self.encoder.encode(self.all_texts, convert_to_tensor=True)\n",
    "\n",
    "    def _get_relevant_content(self, query: str, top_k: int = 3) -> dict:\n",
    "        \"\"\"Get relevant content based on query\"\"\"\n",
    "        query_embedding = self.encoder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "        similarities = cosine_similarity(\n",
    "            query_embedding.cpu().numpy().reshape(1, -1),\n",
    "            self.content_embeddings.cpu().numpy()\n",
    "        )[0]\n",
    "\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        relevant_content = [self.all_texts[i] for i in top_indices]\n",
    "\n",
    "        return {\n",
    "            'relevant_content': relevant_content,\n",
    "            'similarity_scores': similarities[top_indices]\n",
    "        }\n",
    "\n",
    "    def generate_response(self, query: str) -> str:\n",
    "        \"\"\"Generate response to query with a single cohesive answer\"\"\"\n",
    "        try:\n",
    "            # Get relevant content\n",
    "            content = self._get_relevant_content(query)\n",
    "\n",
    "            # Create prompt with a clear instruction to generate one response\n",
    "            context_str = \"\\n\".join([\n",
    "                f\"[Relevance: {score:.2f}] {text}\"\n",
    "                for text, score in zip(content['relevant_content'], content['similarity_scores'])\n",
    "            ])\n",
    "\n",
    "            prompt = f\"\"\"Based on the following social media trends and songs:\n",
    "\n",
    "    {context_str}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Provide a single, cohesive, and creative response, combining insights from the trends and songs to suggest how to promote face products in an engaging way. If the question is out of context of RAG I want you to give a generative answer with latest relevance.\"\"\"\n",
    "\n",
    "            # Generate response\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            inputs = inputs.to(self.model.device)\n",
    "            \n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=300,\n",
    "                min_length=100,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return response.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "\n",
    "# Load and preprocess the CSV files for initializing the model\n",
    "# Replace with your own CSV loading code\n",
    "latest_trends = pd.read_csv(\"latest_insta_trends.csv\")\n",
    "archived_trends = pd.read_csv(\"archived_insta_trends.csv\")\n",
    "songs_df = pd.read_csv(\"instagram_trending_songs.csv\")\n",
    "\n",
    "# Combine trends (latest and archived)\n",
    "combined_trends_df = pd.concat([latest_trends, archived_trends], ignore_index=True)\n",
    "\n",
    "# Initialize the SocialMediaRAG model\n",
    "rag_system = SocialMediaRAG(combined_trends_df, songs_df)\n",
    "\n",
    "@app.post(\"/generate_response/\")\n",
    "async def generate_response(query: Query):\n",
    "    \"\"\"Endpoint to generate response for the query\"\"\"\n",
    "    response = rag_system.generate_response(query.query)\n",
    "    return {\"response\": response}\n",
    "\n",
    "# Set up ngrok to expose the API\n",
    "# Start the server\n",
    "from threading import Thread\n",
    "import uvicorn\n",
    "\n",
    "def start_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Start ngrok\n",
    "ngrok.set_auth_token(\"YOUR_NGROK_TOKEN\")\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
    "\n",
    "# Start the FastAPI server in a separate thread\n",
    "server_thread = Thread(target=start_server)\n",
    "server_thread.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
