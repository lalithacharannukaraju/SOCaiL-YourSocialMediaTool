{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.115.6)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.34.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\mechg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyngrokQ (from versions: none)\n",
      "ERROR: No matching distribution found for pyngrokQ\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for RAG system\n",
    "!pip uninstall google-generativeai -y\n",
    "!pip install transformers sentence-transformers scikit-learn torch pandas numpy google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.genai import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "# Configure Gemini API\n",
    "# Option 1: Use Colab Secrets (Recommended - more secure)\n",
    "# In Colab: Go to Secrets (key icon) -> Add new secret -> Name: GEMINI_API_KEY -> Value: your_api_key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    print(\"âœ… Using API key from Colab secrets\")\n",
    "except:\n",
    "    # Option 2: Direct input (for testing or if not using Colab)\n",
    "    GEMINI_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your actual API key\n",
    "    print(\"âš ï¸  Using direct API key. Make sure to replace 'YOUR_API_KEY_HERE' with your actual key!\")\n",
    "\n",
    "if not GEMINI_API_KEY or GEMINI_API_KEY == \"YOUR_API_KEY_HERE\":\n",
    "    raise ValueError(\"Please set your GEMINI_API_KEY either via Colab secrets or by replacing the placeholder above\")\n",
    "\n",
    "# Initialize the new Google GenAI client\n",
    "genai_client = Client(api_key=GEMINI_API_KEY)\n",
    "print(\"âœ… Gemini API configured successfully!\")\n",
    "\n",
    "class TwitterRAG:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation system for Twitter trends.\n",
    "    Uses semantic search to retrieve relevant trends and Gemini for generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, twitter_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with Twitter trends data.\n",
    "        \n",
    "        Args:\n",
    "            twitter_df: DataFrame with columns 'Trend' and 'Count'\n",
    "        \"\"\"\n",
    "        print(\"Initializing Twitter RAG system...\")\n",
    "        \n",
    "        # Store DataFrame\n",
    "        self.twitter_df = twitter_df\n",
    "        \n",
    "        # Initialize sentence transformer for semantic search\n",
    "        print(\"Loading sentence transformer model...\")\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Initialize Gemini model for generation\n",
    "        print(\"Initializing Gemini model...\")\n",
    "        self.gen_model = genai_client.models.get(model=\"gemini-pro\")\n",
    "        \n",
    "        # Create embeddings for all trends\n",
    "        print(\"Creating embeddings for Twitter trends...\")\n",
    "        self._create_embeddings()\n",
    "        print(\"Initialization complete!\")\n",
    "    \n",
    "    def _create_embeddings(self):\n",
    "        \"\"\"Create embeddings for all Twitter trends\"\"\"\n",
    "        # Process each trend into a text representation\n",
    "        self.trend_texts = []\n",
    "        for _, row in self.twitter_df.iterrows():\n",
    "            trend = str(row['Trend']) if pd.notna(row['Trend']) else \"\"\n",
    "            count = str(row['Count']) if pd.notna(row['Count']) else \"0\"\n",
    "            # Create a descriptive text for each trend\n",
    "            trend_text = f\"Twitter Trend: {trend} | Tweet Count: {count}\"\n",
    "            self.trend_texts.append(trend_text)\n",
    "        \n",
    "        # Create embeddings for all trends\n",
    "        print(f\"Encoding {len(self.trend_texts)} trends...\")\n",
    "        self.trend_embeddings = self.encoder.encode(\n",
    "            self.trend_texts, \n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        print(\"Embeddings created successfully!\")\n",
    "    \n",
    "    def _retrieve_relevant_trends(self, query: str, top_k: int = 5) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve top-k most relevant Twitter trends based on semantic similarity.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question or query\n",
    "            top_k: Number of top trends to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with trend info and similarity scores\n",
    "        \"\"\"\n",
    "        # Encode the query\n",
    "        query_embedding = self.encoder.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity(\n",
    "            query_embedding.cpu().numpy().reshape(1, -1),\n",
    "            self.trend_embeddings.cpu().numpy()\n",
    "        )[0]\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        # Retrieve relevant trends with their metadata\n",
    "        relevant_trends = []\n",
    "        for idx in top_indices:\n",
    "            trend_info = {\n",
    "                'trend': self.twitter_df.iloc[idx]['Trend'],\n",
    "                'count': self.twitter_df.iloc[idx]['Count'],\n",
    "                'similarity': float(similarities[idx]),\n",
    "                'text': self.trend_texts[idx]\n",
    "            }\n",
    "            relevant_trends.append(trend_info)\n",
    "        \n",
    "        return relevant_trends\n",
    "    \n",
    "    def generate_response(self, query: str, top_k: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using RAG: Retrieve relevant trends and generate answer.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            top_k: Number of trends to retrieve for context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Retrieve relevant trends (Retrieval)\n",
    "            print(f\"\\nðŸ” Retrieving top {top_k} relevant Twitter trends...\")\n",
    "            relevant_trends = self._retrieve_relevant_trends(query, top_k)\n",
    "            \n",
    "            # Display retrieved trends\n",
    "            print(\"\\nðŸ“Š Retrieved Trends:\")\n",
    "            for i, trend_info in enumerate(relevant_trends, 1):\n",
    "                print(f\"{i}. {trend_info['trend']} (Count: {trend_info['count']}, Similarity: {trend_info['similarity']:.3f})\")\n",
    "            \n",
    "            # Step 2: Build context from retrieved trends\n",
    "            context_str = \"\\n\".join([\n",
    "                f\"- {trend_info['trend']} (Tweet Count: {trend_info['count']})\"\n",
    "                for trend_info in relevant_trends\n",
    "            ])\n",
    "            \n",
    "            # Step 3: Create prompt with context (Augmentation)\n",
    "            prompt = f\"\"\"You are a social media expert analyzing Twitter trends. Based on the following current Twitter trends data:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Provide a helpful, accurate, and insightful answer based on the Twitter trends data above. If the question relates to the trends, use the specific trend information. If the question is general or not directly related to the trends, provide a generative answer that's relevant and helpful.\n",
    "\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            # Step 4: Generate response using Gemini (Generation)\n",
    "            print(\"\\nðŸ¤– Generating response with Gemini...\")\n",
    "            response = self.gen_model.generate_content(prompt)\n",
    "            \n",
    "            return response.text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def get_trend_summary(self) -> str:\n",
    "        \"\"\"Get a summary of all trends\"\"\"\n",
    "        total_trends = len(self.twitter_df)\n",
    "        top_trends = self.twitter_df.nlargest(5, 'Count')\n",
    "        summary = f\"Total Trends: {total_trends}\\n\\nTop 5 Trends by Count:\\n\"\n",
    "        for idx, row in top_trends.iterrows():\n",
    "            summary += f\"- {row['Trend']}: {row['Count']}\\n\"\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter trends data\n",
    "# Upload twitter_scraper.csv to Colab or mount Google Drive\n",
    "# Option 1: Upload file directly in Colab\n",
    "# Option 2: Mount Google Drive (uncomment below)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Load the CSV file\n",
    "# If uploaded directly, use: 'twitter_scraper.csv'\n",
    "# If from Drive, use: '/content/drive/MyDrive/path/to/twitter_scraper.csv'\n",
    "twitter_df = pd.read_csv('twitter_scraper.csv')\n",
    "\n",
    "print(f\"âœ… Loaded {len(twitter_df)} Twitter trends\")\n",
    "print(f\"\\nFirst few trends:\")\n",
    "print(twitter_df.head())\n",
    "\n",
    "# Initialize the RAG system\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "rag_system = TwitterRAG(twitter_df)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get summary of trends\n",
    "print(\"ðŸ“ˆ Twitter Trends Summary:\")\n",
    "print(rag_system.get_trend_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries to test the RAG system\n",
    "# Try your own questions!\n",
    "\n",
    "# Example 1: Ask about specific trends\n",
    "query1 = \"What are the top trending topics on Twitter right now?\"\n",
    "print(\"=\"*70)\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"=\"*70)\n",
    "response1 = rag_system.generate_response(query1, top_k=5)\n",
    "print(f\"\\nðŸ’¬ Response:\\n{response1}\\n\")\n",
    "\n",
    "# Example 2: Ask about a specific topic\n",
    "query2 = \"What's trending about technology?\"\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"=\"*70)\n",
    "response2 = rag_system.generate_response(query2, top_k=5)\n",
    "print(f\"\\nðŸ’¬ Response:\\n{response2}\\n\")\n",
    "\n",
    "# Example 3: General question\n",
    "query3 = \"How can I create content based on current Twitter trends?\"\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Query: {query3}\")\n",
    "print(\"=\"*70)\n",
    "response3 = rag_system.generate_response(query3, top_k=5)\n",
    "print(f\"\\nðŸ’¬ Response:\\n{response3}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query - Ask your own questions!\n",
    "# Modify the query below to ask anything about Twitter trends\n",
    "\n",
    "your_query = \"What trends are related to politics?\"  # Change this to your question\n",
    "top_k_trends = 5  # Number of trends to retrieve\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Your Query: {your_query}\")\n",
    "print(\"=\"*70)\n",
    "response = rag_system.generate_response(your_query, top_k=top_k_trends)\n",
    "print(f\"\\nðŸ’¬ Response:\\n{response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
